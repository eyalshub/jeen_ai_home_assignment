# ğŸ§  Jeen.ai Home Assignment â€“ Semantic Search RAG Simulation

## ğŸ” Introduction

As part of the Jeen.ai Home Assignment, I chose to design and implement a **modular, testable, and extensible RAG pipeline** to enable easy future enhancements and experimentation.

### ğŸ’¡ Key Design Decisions
- ğŸ”§ **Modular Architecture** â€“ Each component (e.g., chunking, embedding, database) is isolated and swappable.
- ğŸ§ª **Full Test Coverage** â€“ Unit tests were written for all core functionalities to ensure correctness and stability.
- ğŸ³ **Dockerized Environment** â€“ PostgreSQL with `pgvector` runs inside Docker for easy setup and portability across machines.
- â™»ï¸ **Connection Pooling** â€“ Efficient and safe PostgreSQL connections via `ThreadedConnectionPool`.

> ğŸ¯ The goal was to build a clean, maintainable, and production-ready solution â€” not just a proof-of-concept script.

---

This project implements a minimal **RAG (Retrieval-Augmented Generation)** pipeline using:

- ğŸ§  **Gemini API** for embedding generation  
- ğŸ—ƒï¸ **PostgreSQL + pgvector** for vector storage and semantic search

It consists of two main scripts:

- `index_documents.py` â€“ Extracts text from documents, chunks it, generates embeddings, and stores them in the database.
- `search_documents.py` â€“ Embeds a user query and retrieves the top 5 most semantically similar chunks using cosine similarity.


> ğŸ“ _This project uses CLI only â€“ no UI or graphical components included._

---

## ğŸ“¦ Features

- âœ… **Embedding generation with Gemini API**  
  Leverages Google's Gemini model to create high-dimensional vector representations of text.

- âœ… **Semantic search with PostgreSQL + pgvector**  
  Uses `pgvector` extension for efficient similarity search via cosine distance.

- âœ… **Flexible chunking strategies**  
  Supports:
  - `fixed` â€“ splits by fixed word count  
  - `sentence` â€“ splits by sentence boundaries  
  - `paragraph` â€“ splits by paragraph markers

- âœ… **Secure configuration**  
  Environment variables (API keys, DB credentials) are stored safely in a `.env` file.

- âœ… **Robust testing**  
  Includes over 10 edge-case unit tests written with `pytest` for confidence in core logic.

- âœ… **Clean, modular codebase**  
  Organized into logical components (`embedder`, `chunker`, `database`, etc.) for easy maintenance and extension.

- âœ… **Efficient connection management**  
  Uses `psycopg2.pool.ThreadedConnectionPool` to safely handle multiple concurrent DB queries without exhausting connections.

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository & set up environment

```bash
git clone https://github.com/your_username/jeen_ai_home_assignment.git
cd jeen_ai_home_assignment

python -m venv .venv
.venv\Scripts\activate  # On Windows
# source .venv/bin/activate  # On Mac/Linux

pip install -r requirements.txt
```



Create a file named `.env` in the project root and copy the following template:

# Gemini API
GEMINI_API_KEY=your_gemini_api_key_here
...
# PostgreSQL
POSTGRES_DB=jeen_ai
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_HOST=localhost
POSTGRES_PORT=5432



âš ï¸ Make sure PostgreSQL is running and the pgvector extension is installed.
```bash
pytest test_search_documents.py -v
```
This will run all 10+ test cases, including:

- Embedding-based similarity sorting  
- Top-K result limiting  
- Filtering by `filename` and `split_strategy`  
- Score range validation  
- Handling of edge cases like blank queries or empty database


python index_documents.py samples/sample.docx --strategy fixed

Supported formats:
- .docx
- .pdf

Supported chunking strategies:
- fixed â€“ chunks by word count
- sentence â€“ sentence-level split
- paragraph â€“ paragraph-level split


from search_documents import search_documents

results = search_documents("What is semantic search?")
for r in results:
    print(r["chunk_text"], "â†’ score:", r["score"])


---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ index_documents.py               # CLI script: process and index documents
â”œâ”€â”€ search_documents.py              # CLI script: semantic search interface
â”œâ”€â”€ docker-compose.yml               # PostgreSQL + pgvector setup
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .env                             # Environment variables (not tracked by Git)
â”œâ”€â”€ .gitignore                       # Git exclusions
â”œâ”€â”€ README.md                        # Project documentation

â”œâ”€â”€ helper/                          # Core logic modules
â”‚   â”œâ”€â”€ chunker.py                   # Text splitting strategies (fixed, sentence, paragraph)
â”‚   â”œâ”€â”€ embedder.py                 # Gemini-based embedding generation
â”‚   â”œâ”€â”€ extractor.py                # PDF and DOCX file text extraction
â”‚   â””â”€â”€ database.py                 # DB schema, insertion, querying, pooling
â”‚   â””â”€â”€ reset_db.py                 #   Cleanly restart the database
â”‚   â””â”€â”€ setup_db.py                 #  Initialize the database   
â”œâ”€â”€ tests/                           # Unit tests (pytest)
â”‚   â”œâ”€â”€ test_chunker.py
â”‚   â”œâ”€â”€ test_embedder.py
â”‚   â”œâ”€â”€ test_extractor.py
â”‚   â”œâ”€â”€ test_database.py
â”‚   â”œâ”€â”€ test_index_documents.py
â”‚   â””â”€â”€ test_search_documents.py

â”œâ”€â”€ samples/                         # Example documents for testing
â”‚   â”œâ”€â”€ sample.docx
â”‚   â”œâ”€â”€ file-sample_150kB.pdf
â”‚   â””â”€â”€ file-sample_500kB.docx
â”‚   â””â”€â”€ Generative_AI.docx
â”‚   â””â”€â”€ sample.docx
â”‚   â””â”€â”€ ai_overview_long.pdf
â”‚   â””â”€â”€ encoding_vs_decoding.docx

â”œâ”€â”€ photos/                          # Photos from the launch and all stages of the project
â”‚   â””â”€â”€ jeen.....
```

---

## ğŸ›¡ï¸ Security

- âœ… Environment variables are stored in a `.env` file (excluded from Git)
- âœ… API keys and DB credentials are never exposed in code
- âœ… Secure handling of external API calls (Gemini)
- âœ… Exception handling with detailed logging  
  All core functions log errors gracefully with context-aware messages (e.g., failed chunk insertions).

---

## ğŸ™Œ Credits

Developed by **Eyal Shubeli**  
_As part of the Jeen.ai AI Solutions Engineer Home Assignment â€“ Stage 2._

Includes:

- Chunk-based document indexing  
- Embedding generation using Google Gemini API  
- Cosine similarity search using `pgvector`  
- End-to-end unit testing with `pytest`  


## ğŸ§ª Two Ways to Use It: Quick CLI vs. Python Import
### Quick CLI
ğŸ“˜ Example Usage (Simplified Flow)
ğŸ§± Step 1: Start the database (Docker)
Make sure Docker is running, then launch PostgreSQL with pgvector:
docker-compose up -d
This will start a PostgreSQL container with the pgvector extension ready.

ğŸ“„ Step 2: Index a document:
Use the CLI to extract, chunk, embed, and store a file into the database:
python index_documents.py samples/sample.docx --strategy fixed 
Supported formats:
* .docx
* .pdf

Supported chunking strategies:
* fixed
* sentence
* paragraph

âœ… Output:
ğŸ“„ Reading file: sample.docx
âœ‚ï¸ Splitting text using strategy: fixed
ğŸ§  Generating embeddings...
ğŸ’¾ Saving chunks to DB...
âœ… Done!

ğŸ” Step 3: Run a semantic search query
Ask a natural language question using:
python search_documents.py "What is semantic search?"
âœ… Output:
Semantic search is a powerful technique...   â†’ score: 0.812
This approach powers recommendation...       â†’ score: 0.792


ğŸ§¼ Optional: Reset the database
You can clear previous runs with:
python helper/reset_db.py


### Python Import
ğŸ“˜the other way Example Usage 
ğŸ§¾ Step 1: Prepare a sample document

Save your document (e.g., sample.docx) inside the samples/ folder.

samples/
â””â”€â”€ sample.docx   # Contains 2â€“3 paragraphs of educational content


Example contents for sample.docx:

Semantic search is a powerful technique that enables machines to understand the meaning behind words rather than relying solely on keyword matching.
By using embeddings, large language models and vector databases can represent text as high-dimensional vectors.
This approach powers recommendation systems, chatbots, and modern document search engines.

ğŸ§  Step 2: Index the document

Run the following script to chunk the document and save it to the database:

python index_documents.py samples/sample.docx --strategy fixed


âœ… Example output:

ğŸ“„ Reading file: sample.docx
âœ‚ï¸ Splitting text using strategy: fixed
ğŸ§  Generating embeddings for 2 chunks...
ğŸ’¾ Saving 2 chunks to DB...
âœ… Done!

ğŸ” Step 3: Run a semantic search query

You can now search semantically using a Python script or an interactive shell:

from search_documents import search_documents

results = search_documents("What is semantic search?")
for r in results:
    print(r["chunk_text"], round(r["score"], 3))


ğŸ§ª Example output:

Semantic search is a powerful technique...                    0.813
...The effectiveness of such systems relies...               0.792
Chunk with sim 0.99                                          -0.002
Chunk with sim 0.95                                          -0.002
Chunk with sim 0.75                                          -0.002

ğŸ§¼ Resetting the database (optional)

If you want to clean the database between runs (for testing/demo):

from database import get_connection
with get_connection() as conn:
    with conn.cursor() as cur:
        cur.execute("DELETE FROM chunks;")
        conn.commit()


<img width="784" height="410" alt="image" src="https://github.com/user-attachments/assets/a77d00b3-82f2-4d38-bd77-edd3146ff760" />
